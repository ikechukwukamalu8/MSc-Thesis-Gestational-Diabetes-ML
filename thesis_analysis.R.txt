# ==============================================================================
# MSc Thesis: Impact of Data Splitting on ML Performance
# Script: Data Preprocessing
# Author: Ikechukwu Okechi Kamalu
# ==============================================================================
library(tidyverse)
library(readxl)

# Load dataset
DATA <- read_excel("dataCBGS_dataset.xlsx")

# Basic preprocessing
DATA <- DATA %>%
  select(-Dummy_Study_Number) %>%
  mutate(
    Gestational_diabetes = factor(Gestational_diabetes, levels = c("No", "Yes")),
    sex_of_the_baby = factor(sex_of_the_baby),
    across(where(is.character), as.factor)
  )

# Save cleaned data
saveRDS(DATA, "data/clean_data.rds")

# ==============================================================================
# Script: Multiple Imputation using MICE
# ==============================================================================

library(mice)

DATA <- readRDS("data/clean_data.rds")

set.seed(123)
imputation_model <- mice(
  DATA,
  m = 5,
  method = "pmm",
  maxit = 30,
  seed = 123
)

# Extract completed dataset
DATA_COMPLETE <- complete(imputation_model)

saveRDS(DATA_COMPLETE, "data/imputed_data.rds")

# ==============================================================================
# Script: Model Training and Evaluation under Different Data Splits
# ==============================================================================

library(caret)
library(pROC)
library(ROSE)
library(randomForest)
library(e1071)
library(naivebayes)
library(glmnet)
library(xgboost)

DATA <- readRDS("data/imputed_data.rds")

splits <- c(0.66, 0.70, 0.75, 0.80)

train_ctrl <- trainControl(
  method = "none",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

evaluate_model <- function(method, split_ratio) {

  set.seed(123)
  idx <- createDataPartition(DATA$Gestational_diabetes, p = split_ratio, list = FALSE)
  train_data <- DATA[idx, ]
  test_data  <- DATA[-idx, ]

  # Balance training data ONLY
  train_bal <- ROSE(Gestational_diabetes ~ ., data = train_data)$data

  model_fit <- train(
    Gestational_diabetes ~ .,
    data = train_bal,
    method = method,
    metric = "ROC",
    trControl = train_ctrl
  )

  probs <- predict(model_fit, test_data, type = "prob")[,"Yes"]
  roc_obj <- roc(test_data$Gestational_diabetes, probs)

  return(list(AUC = auc(roc_obj), ROC = roc_obj))
}

models <- c(
  Logistic = "glmnet",
  SVM = "svmRadial",
  RandomForest = "rf",
  NaiveBayes = "naive_bayes",
  KNN = "knn",
  XGBoost = "xgbTree"
)

results <- list()

for (m in names(models)) {
  for (s in splits) {
    key <- paste(m, s, sep = "_")
    results[[key]] <- evaluate_model(models[m], s)
  }
}

saveRDS(results, "results/model_results.rds")

# ==============================================================================
# Script: Results Summary and ROC Visualization
# ==============================================================================

library(tidyverse)
library(pROC)
library(ggplot2)

results <- readRDS("results/model_results.rds")

auc_table <- tibble(
  Model_Split = names(results),
  AUC = sapply(results, function(x) as.numeric(x$AUC))
)

write.csv(auc_table, "results/auc_results.csv", row.names = FALSE)

roc_df <- bind_rows(lapply(names(results), function(k) {
  r <- results[[k]]$ROC
  data.frame(
    FPR = 1 - r$specificities,
    TPR = r$sensitivities,
    Model = k
  )
}))

p <- ggplot(roc_df, aes(FPR, TPR, color = Model)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed") +
  theme_minimal() +
  labs(title = "ROC Curves Across Models and Data Splits")

ggsave("results/roc_curves.png", plot = p, width = 10, height = 7)

sessionInfo()

